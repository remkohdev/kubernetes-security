{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Kubernetes Security \u00b6 Kubernetes Networking \u00b6 Kubernetes Networking 101 (60 mins), you will use different ways to control traffic on a Kubernetes cluster with Service types, Ingress, Network Policy and Calico. Start here . Kubernetes Network Security using a Virtual Private Cloud (VPC) (90 mins), you will deploy a guestbook application to a Kubernetes cluster in a Virtual Private Cloud (VPC) Gen2, you will create the VPC, add a subnet, attach a public gateway, and update a security group with rules to allow inbound traffic to the guestbook application. Start here . Istio , use Istio to manage network traffic, load balance across microservices, enforce access policies, verify service identity, and more. Configuration Management \u00b6 Application Configuration for Kubernetes: Lab0. Setup Lab1. Container Configuration Lab2. Using Environment Variables in Pod Config Lab3. Store Key-Value Pairs using ConfigMap Lab4. Store Sensitive Data using Secrets Lab5. Pull an Image from a Private Registry Key Management Services (KMS) Lab1. Encrypt Secrets using a Cloud-Managed Vault Service (IBM Secrets Manager) Lab 2. Using Vault on OpenShift Lab2. Setup Internal Vault with Vault Agent Injector on OpenShift Lab3. Access Internal Vault using Vault Agent Injector","title":"About the workshop"},{"location":"#kubernetes-security","text":"","title":"Kubernetes Security"},{"location":"#kubernetes-networking","text":"Kubernetes Networking 101 (60 mins), you will use different ways to control traffic on a Kubernetes cluster with Service types, Ingress, Network Policy and Calico. Start here . Kubernetes Network Security using a Virtual Private Cloud (VPC) (90 mins), you will deploy a guestbook application to a Kubernetes cluster in a Virtual Private Cloud (VPC) Gen2, you will create the VPC, add a subnet, attach a public gateway, and update a security group with rules to allow inbound traffic to the guestbook application. Start here . Istio , use Istio to manage network traffic, load balance across microservices, enforce access policies, verify service identity, and more.","title":"Kubernetes Networking"},{"location":"#configuration-management","text":"Application Configuration for Kubernetes: Lab0. Setup Lab1. Container Configuration Lab2. Using Environment Variables in Pod Config Lab3. Store Key-Value Pairs using ConfigMap Lab4. Store Sensitive Data using Secrets Lab5. Pull an Image from a Private Registry Key Management Services (KMS) Lab1. Encrypt Secrets using a Cloud-Managed Vault Service (IBM Secrets Manager) Lab 2. Using Vault on OpenShift Lab2. Setup Internal Vault with Vault Agent Injector on OpenShift Lab3. Access Internal Vault using Vault Agent Injector","title":"Configuration Management"},{"location":"kubeconfig/","text":"Application Configuration for Kubernetes \u00b6 Lab1: Container Configuration (tbd) Lab2: Using Environment Variables in Pod Config Lab3: Store Key-Value Pairs using ConfigMap Lab4: Store Sensitive Data using Secrets Lab5: Pull an Image from a Private Registry","title":"Application Configuration for Kubernetes"},{"location":"kubeconfig/#application-configuration-for-kubernetes","text":"Lab1: Container Configuration (tbd) Lab2: Using Environment Variables in Pod Config Lab3: Store Key-Value Pairs using ConfigMap Lab4: Store Sensitive Data using Secrets Lab5: Pull an Image from a Private Registry","title":"Application Configuration for Kubernetes"},{"location":"kubeconfig/0_setup/","text":"Setup \u00b6 Pre-requirements \u00b6 Client terminal [Optional] Client terminal with Docker daemon to build Docker image, Helm v3 OpenShift cluster Connect to OpenShift \u00b6 oc login oc new-project my-vault Install Helm v3 \u00b6 In IBM Cloud Shell you can use an alias for Helm3, alias helm = helm3 wget https://get.helm.sh/helm-v3.2.0-linux-amd64.tar.gz tar -zxvf helm-v3.2.0-linux-amd64.tar.gz echo 'export PATH=$HOME/linux-amd64:$PATH' > .bash_profile source .bash_profile helm version --short Install MongoDB to OpenShift \u00b6 Find Supplemental Group ID for Namespace \u00b6 During the creation of a project or namespace, OpenShift assigns a User ID (UID) range, a supplemental group ID (GID) range, and unique SELinux MCS labels to the project or namespace. By default, no range is explicitly defined for fsGroup. The supplemental Groups IDs are used for controlling access to shared storage like NFS and GlusterFS, while the fsGroup is used for controlling access to block storage such as Ceph RBD, iSCSI, and some Cloud storage. https://github.com/bitnami/charts/blob/master/bitnami/mongodb/values.yaml The fsGroup of a PodSecurityPolicy controls the supplemental group applied to some volumes: MustRunAs , MayRunAs or RunAsAny . Change the owner and group of the persistent volume(s) mountpoint(s) to 'runAsUser:fsGroup' on each component, values from the securityContext section of the component On OpenShift, Admission control with SCCs allows for control over the creation of resources based on the capabilities granted to a user. The set of SCCs that admission uses to authorize a pod are determined by the user identity and groups that the user belongs to. If the pod specifies a service account, the set of allowable SCCs includes any constraints accessible to the service account. For your namespace get the retrieve the supplemental Group ID and the User ID, $ oc get project my-vault -o yaml | grep 'sa.scc.' openshift.io/sa.scc.mcs: s0:c25,c10 openshift.io/sa.scc.supplemental-groups: 1000620000 /10000 openshift.io/sa.scc.uid-range: 1000620000 /10000 f:openshift.io/sa.scc.mcs: {} f:openshift.io/sa.scc.supplemental-groups: {} f:openshift.io/sa.scc.uid-range: {} defines that the sa.scc.supplemental-groups allowed are 1000620000/10000, the sa.scc.uid-range for the project is 1000620000/10000 in format M/N, where M is the starting ID and N is the count. Using the fsGroup and user ids, create two environment variables, SA_SCC_FSGROUP = <value of sa.scc.supplemental-groups> SA_SCC_RUNASUSER = <value of sa.scc.uid-range> Deploy Mongodb \u00b6 helm repo add bitnami https://charts.bitnami.com/bitnami helm repo update MONGO_USER = mongoadmin MONGO_PASS = m0n90s3cr3t MONGO_PORT = 27017 MONGO_DB = entries MONGO_AUTH_DB = admin helm install mongodb bitnami/mongodb --set persistence.enabled = false --set livenessProbe.initialDelaySeconds = 180 --set auth.rootPassword = $MONGO_PASS --set auth.username = $MONGO_USER --set auth.password = $MONGO_PASS --set auth.database = $MONGO_DB --set service.type = ClusterIP --set podSecurityContext.enabled = true,podSecurityContext.fsGroup = $SA_SCC_FSGROUP ,containerSecurityContext.enabled = true,containerSecurityContext.runAsUser = $SA_SCC_RUNASUSER Access the mongo container's terminal and test access to mongodb, $ oc get pods NAME READY STATUS RESTARTS AGE mongodb-8444497695-79rf9 1 /1 Running 0 21m $ oc exec -it mongodb-8444497695-79rf9 -- bash > mongo --host 127 .0.0.1 --authenticationDatabase admin -p $MONGODB_ROOT_PASSWORD Or use port-forwarding oc port-forward --namespace my-vault svc/mongodb 27017 :27017 & mongo --host 127 .0.0.1 --authenticationDatabase admin -p $MONGODB_ROOT_PASSWORD exit pkill kubectl -9 The output of the helm install mongodb should be, $ helm install mongodb bitnami/mongodb --set persistence. enabled = false --set livenessProbe.initialDelaySeconds = 180 --set auth.rootPasswo rd = $MONGO_PASS --set auth.username = $MONGO_USER --set auth.password = $MONGO_PASS --set auth.database = $MONGO_DB --set service.type = ClusterIP --set podSecurityCon text.enabled = true,podSecurityContext.fsGroup = $SA_SCC_FSGROUP ,containerSecurityC ontext.enabled = true,containerSecurityContext.runAsUser = $SA_SCC_RUNASUSER NAME: mongodb LAST DEPLOYED: Tue Mar 9 21 :28:18 2021 NAMESPACE: my-vault STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: ** Please be patient while the chart is being deployed ** MongoDB ( R ) can be accessed on the following DNS name ( s ) and ports from within y our cluster: mongodb.my-vault.svc.cluster.local To get the root password run: export MONGODB_ROOT_PASSWORD = $( kubectl get secret --namespace my-vault mong odb -o jsonpath = \"{.data.mongodb-root-password}\" | base64 --decode ) To get the password for \"mongoadmin\" run: export MONGODB_PASSWORD = $( kubectl get secret --namespace my-vault mongodb - o jsonpath = \"{.data.mongodb-password}\" | base64 --decode ) To connect to your database, create a MongoDB ( R ) client container: kubectl run --namespace my-vault mongodb-client --rm --tty -i --restart = 'Never' --env = \"MONGODB_ROOT_PASSWORD= $MONGODB_ROOT_PASSWORD \" --image docker.io/bitnami/mongodb:4.4.4-debian-10-r0 --command -- bash Then, run the following command: mongo admin --host \"mongodb\" --authenticationDatabase admin -u root -p $MONGODB_ROOT_PASSWORD To connect to your database from outside the cluster execute the following commands: kubectl port-forward --namespace my-vault svc/mongodb 27017 :27017 & mongo --host 127 .0.0.1 --authenticationDatabase admin -p $MONGODB_ROOT_PASSWORD Optional verify the credentials set to access MongoDB, MONGODB_ROOT_PASSWORD = $( oc get secret mongodb -o jsonpath = \"{.data.mongodb-root-password}\" | base64 --decode ) echo $MONGODB_ROOT_PASSWORD MONGODB_PASSWORD = $( oc get secret mongodb -o jsonpath = \"{.data.mongodb-password}\" | base64 --decode ) echo $MONGODB_PASSWORD","title":"Lab0. Setup"},{"location":"kubeconfig/0_setup/#setup","text":"","title":"Setup"},{"location":"kubeconfig/0_setup/#pre-requirements","text":"Client terminal [Optional] Client terminal with Docker daemon to build Docker image, Helm v3 OpenShift cluster","title":"Pre-requirements"},{"location":"kubeconfig/0_setup/#connect-to-openshift","text":"oc login oc new-project my-vault","title":"Connect to OpenShift"},{"location":"kubeconfig/0_setup/#install-helm-v3","text":"In IBM Cloud Shell you can use an alias for Helm3, alias helm = helm3 wget https://get.helm.sh/helm-v3.2.0-linux-amd64.tar.gz tar -zxvf helm-v3.2.0-linux-amd64.tar.gz echo 'export PATH=$HOME/linux-amd64:$PATH' > .bash_profile source .bash_profile helm version --short","title":"Install Helm v3"},{"location":"kubeconfig/0_setup/#install-mongodb-to-openshift","text":"","title":"Install MongoDB to OpenShift"},{"location":"kubeconfig/0_setup/#find-supplemental-group-id-for-namespace","text":"During the creation of a project or namespace, OpenShift assigns a User ID (UID) range, a supplemental group ID (GID) range, and unique SELinux MCS labels to the project or namespace. By default, no range is explicitly defined for fsGroup. The supplemental Groups IDs are used for controlling access to shared storage like NFS and GlusterFS, while the fsGroup is used for controlling access to block storage such as Ceph RBD, iSCSI, and some Cloud storage. https://github.com/bitnami/charts/blob/master/bitnami/mongodb/values.yaml The fsGroup of a PodSecurityPolicy controls the supplemental group applied to some volumes: MustRunAs , MayRunAs or RunAsAny . Change the owner and group of the persistent volume(s) mountpoint(s) to 'runAsUser:fsGroup' on each component, values from the securityContext section of the component On OpenShift, Admission control with SCCs allows for control over the creation of resources based on the capabilities granted to a user. The set of SCCs that admission uses to authorize a pod are determined by the user identity and groups that the user belongs to. If the pod specifies a service account, the set of allowable SCCs includes any constraints accessible to the service account. For your namespace get the retrieve the supplemental Group ID and the User ID, $ oc get project my-vault -o yaml | grep 'sa.scc.' openshift.io/sa.scc.mcs: s0:c25,c10 openshift.io/sa.scc.supplemental-groups: 1000620000 /10000 openshift.io/sa.scc.uid-range: 1000620000 /10000 f:openshift.io/sa.scc.mcs: {} f:openshift.io/sa.scc.supplemental-groups: {} f:openshift.io/sa.scc.uid-range: {} defines that the sa.scc.supplemental-groups allowed are 1000620000/10000, the sa.scc.uid-range for the project is 1000620000/10000 in format M/N, where M is the starting ID and N is the count. Using the fsGroup and user ids, create two environment variables, SA_SCC_FSGROUP = <value of sa.scc.supplemental-groups> SA_SCC_RUNASUSER = <value of sa.scc.uid-range>","title":"Find Supplemental Group ID for Namespace"},{"location":"kubeconfig/0_setup/#deploy-mongodb","text":"helm repo add bitnami https://charts.bitnami.com/bitnami helm repo update MONGO_USER = mongoadmin MONGO_PASS = m0n90s3cr3t MONGO_PORT = 27017 MONGO_DB = entries MONGO_AUTH_DB = admin helm install mongodb bitnami/mongodb --set persistence.enabled = false --set livenessProbe.initialDelaySeconds = 180 --set auth.rootPassword = $MONGO_PASS --set auth.username = $MONGO_USER --set auth.password = $MONGO_PASS --set auth.database = $MONGO_DB --set service.type = ClusterIP --set podSecurityContext.enabled = true,podSecurityContext.fsGroup = $SA_SCC_FSGROUP ,containerSecurityContext.enabled = true,containerSecurityContext.runAsUser = $SA_SCC_RUNASUSER Access the mongo container's terminal and test access to mongodb, $ oc get pods NAME READY STATUS RESTARTS AGE mongodb-8444497695-79rf9 1 /1 Running 0 21m $ oc exec -it mongodb-8444497695-79rf9 -- bash > mongo --host 127 .0.0.1 --authenticationDatabase admin -p $MONGODB_ROOT_PASSWORD Or use port-forwarding oc port-forward --namespace my-vault svc/mongodb 27017 :27017 & mongo --host 127 .0.0.1 --authenticationDatabase admin -p $MONGODB_ROOT_PASSWORD exit pkill kubectl -9 The output of the helm install mongodb should be, $ helm install mongodb bitnami/mongodb --set persistence. enabled = false --set livenessProbe.initialDelaySeconds = 180 --set auth.rootPasswo rd = $MONGO_PASS --set auth.username = $MONGO_USER --set auth.password = $MONGO_PASS --set auth.database = $MONGO_DB --set service.type = ClusterIP --set podSecurityCon text.enabled = true,podSecurityContext.fsGroup = $SA_SCC_FSGROUP ,containerSecurityC ontext.enabled = true,containerSecurityContext.runAsUser = $SA_SCC_RUNASUSER NAME: mongodb LAST DEPLOYED: Tue Mar 9 21 :28:18 2021 NAMESPACE: my-vault STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: ** Please be patient while the chart is being deployed ** MongoDB ( R ) can be accessed on the following DNS name ( s ) and ports from within y our cluster: mongodb.my-vault.svc.cluster.local To get the root password run: export MONGODB_ROOT_PASSWORD = $( kubectl get secret --namespace my-vault mong odb -o jsonpath = \"{.data.mongodb-root-password}\" | base64 --decode ) To get the password for \"mongoadmin\" run: export MONGODB_PASSWORD = $( kubectl get secret --namespace my-vault mongodb - o jsonpath = \"{.data.mongodb-password}\" | base64 --decode ) To connect to your database, create a MongoDB ( R ) client container: kubectl run --namespace my-vault mongodb-client --rm --tty -i --restart = 'Never' --env = \"MONGODB_ROOT_PASSWORD= $MONGODB_ROOT_PASSWORD \" --image docker.io/bitnami/mongodb:4.4.4-debian-10-r0 --command -- bash Then, run the following command: mongo admin --host \"mongodb\" --authenticationDatabase admin -u root -p $MONGODB_ROOT_PASSWORD To connect to your database from outside the cluster execute the following commands: kubectl port-forward --namespace my-vault svc/mongodb 27017 :27017 & mongo --host 127 .0.0.1 --authenticationDatabase admin -p $MONGODB_ROOT_PASSWORD Optional verify the credentials set to access MongoDB, MONGODB_ROOT_PASSWORD = $( oc get secret mongodb -o jsonpath = \"{.data.mongodb-root-password}\" | base64 --decode ) echo $MONGODB_ROOT_PASSWORD MONGODB_PASSWORD = $( oc get secret mongodb -o jsonpath = \"{.data.mongodb-password}\" | base64 --decode ) echo $MONGODB_PASSWORD","title":"Deploy Mongodb"},{"location":"kubeconfig/1_container_config/","text":"Lab1: Container Configuration \u00b6 tbd","title":"Lab1. Container Config"},{"location":"kubeconfig/1_container_config/#lab1-container-configuration","text":"tbd","title":"Lab1: Container Configuration"},{"location":"kubeconfig/2_config_using_env/","text":"Lab2: Using Environment Variables in Pod Config \u00b6 To set environment variables for the containers running in the Pod, you can use the env or envFrom field in the configuration. Build the Guestbook App \u00b6 You can use a prebuilt image for Guestbook, available at remkohdev/guestbook-nodejs:1.0.0 or using a private registry at quay.io/remkohdev/guestbook-nodejs:1.0.0 . Optionally, you can build your own container image from source and and push it to your personal container registry. git clone https://github.com/IBM/guestbook-nodejs.git cd guestbook-nodejs/src DOCKER_USER = <your Docker username> docker login -u $DOCKER_USER docker build -t guestbook-nodejs:1.0.0 . docker tag guestbook-nodejs:1.0.0 $DOCKER_USER /guestbook-nodejs:1.0.0 docker push $DOCKER_USER /guestbook-nodejs:1.0.0 Deploy the Guestbook app \u00b6 If you use the pre-built image, set the following environment variable to remkohdev , DOCKER_USER = remkohdev Guestbook is built on top of Loopback . By default, it uses an in-memory database. A datasource is defined in server/datasources.json and the model is connected to the datasource in server/model-config.json . To define an in-memory or other datasource explicitly, you can pass the NODE_ENV environment variable, which makes Loopback look for a datasources.<$NODE_ENV>.json and model-config.<$NODE_ENV>.json file for configuration. As a first step, create the following Deployment file without environment variables, cat > guestbook-deployment.yaml <<EOF --- apiVersion: apps/v1 kind: Deployment metadata: name: guestbook namespace: my-vault labels: app: guestbook spec: replicas: 1 selector: matchLabels: app: guestbook template: metadata: labels: app: guestbook spec: serviceAccountName: guestbook containers: - name: app image: $DOCKER_USER/guestbook-nodejs:1.0.0 imagePullPolicy: Always ports: - name: http containerPort: 3000 EOF And create a Service resource, cat > guestbook-service.yaml <<EOF apiVersion: v1 kind: Service metadata: name: guestbook labels: app: guestbook spec: type: LoadBalancer ports: - port: 80 targetPort: 3000 name: http selector: app: guestbook EOF Now, create the Guestbook app using the in-memory database, oc create -f guestbook-deployment.yaml oc create -f guestbook-service.yaml oc expose service guestbook ROUTE = $( oc get route guestbook -o json | jq -r '.spec.host' ) echo $ROUTE Test the application by creating a message and read all current messages, curl -X POST http:// $ROUTE /api/entries -H 'Content-Type: application/json' -H 'Accept: application/json' -d '{ \"message\": \"hello1\" }' curl -X GET http:// $ROUTE /api/entries -H 'Accept: application/json' Right now, if you delete and recreate the Deployment for Guestbook, no persistence is configured. Add Environment Variables using Env \u00b6 The NODE_ENV variable is used to localize configuration of Guestbook. Passing an environment variable NODE_ENV=mongo will make the Loopback framework look for configuration files localized for a mongo environment. The datasources and model-config files for a mongo environment are defined in the files datasources.mongo.json and model-config.mongo.json in the server directory. The datasources.mongo.json is configured as follows using environment variables, { \"mongo\" : { \"name\" : \"mongo\" , \"connector\" : \"mongodb\" , \"host\" : \"${MONGO_HOST}\" , \"port\" : \"${MONGO_PORT}\" , \"database\" : \"${MONGO_DB}\" , \"user\" : \"${MONGO_USER}\" , \"password\" : \"${MONGO_PASS}\" , \"url\" : \"\" } } Create a Deployment resource, which includes the expected environment variables to configure the MongoDB connection in the datasources.mongo.json , cat > guestbook-deployment.yaml <<EOF --- apiVersion: apps/v1 kind: Deployment metadata: name: guestbook namespace: my-vault labels: app: guestbook spec: replicas: 1 selector: matchLabels: app: guestbook template: metadata: labels: app: guestbook spec: serviceAccountName: guestbook containers: - name: app image: $DOCKER_USER/guestbook-nodejs:1.0.0 imagePullPolicy: Always ports: - name: http containerPort: 3000 env: - name: NODE_ENV value: \"mongo\" - name: MONGO_HOST value: \"mongodb\" - name: MONGO_PORT value: \"27017\" - name: MONGO_DB value: \"entries\" - name: MONGO_AUTH_DB value: \"admin\" - name: MONGO_USER value: \"mongoadmin\" - name: MONGO_PASS value: \"m0n90s3cr3t\" EOF Delete and recreate the Guestbook Deployment, oc delete deployment guestbook oc create -f guestbook-deployment.yaml Check the environment variables, oc get pods oc exec guestbook-<instance-id> -- printenv Test the application and create a message and read all messages, curl -X POST http:// $ROUTE /api/entries -H 'Content-Type: application/json' -H 'Accept: application/json' -d '{ \"message\": \"hello1\" }' curl -X GET http:// $ROUTE /api/entries -H 'Accept: application/json' To test the database, delete the deployment and create it again, you should now have persisted your messages, oc delete deployment guestbook oc create -f guestbook-deployment.yaml curl -X GET http:// $ROUTE /api/entries -H 'Accept: application/json'","title":"Lab2. Pod Config"},{"location":"kubeconfig/2_config_using_env/#lab2-using-environment-variables-in-pod-config","text":"To set environment variables for the containers running in the Pod, you can use the env or envFrom field in the configuration.","title":"Lab2: Using Environment Variables in Pod Config"},{"location":"kubeconfig/2_config_using_env/#build-the-guestbook-app","text":"You can use a prebuilt image for Guestbook, available at remkohdev/guestbook-nodejs:1.0.0 or using a private registry at quay.io/remkohdev/guestbook-nodejs:1.0.0 . Optionally, you can build your own container image from source and and push it to your personal container registry. git clone https://github.com/IBM/guestbook-nodejs.git cd guestbook-nodejs/src DOCKER_USER = <your Docker username> docker login -u $DOCKER_USER docker build -t guestbook-nodejs:1.0.0 . docker tag guestbook-nodejs:1.0.0 $DOCKER_USER /guestbook-nodejs:1.0.0 docker push $DOCKER_USER /guestbook-nodejs:1.0.0","title":"Build the Guestbook App"},{"location":"kubeconfig/2_config_using_env/#deploy-the-guestbook-app","text":"If you use the pre-built image, set the following environment variable to remkohdev , DOCKER_USER = remkohdev Guestbook is built on top of Loopback . By default, it uses an in-memory database. A datasource is defined in server/datasources.json and the model is connected to the datasource in server/model-config.json . To define an in-memory or other datasource explicitly, you can pass the NODE_ENV environment variable, which makes Loopback look for a datasources.<$NODE_ENV>.json and model-config.<$NODE_ENV>.json file for configuration. As a first step, create the following Deployment file without environment variables, cat > guestbook-deployment.yaml <<EOF --- apiVersion: apps/v1 kind: Deployment metadata: name: guestbook namespace: my-vault labels: app: guestbook spec: replicas: 1 selector: matchLabels: app: guestbook template: metadata: labels: app: guestbook spec: serviceAccountName: guestbook containers: - name: app image: $DOCKER_USER/guestbook-nodejs:1.0.0 imagePullPolicy: Always ports: - name: http containerPort: 3000 EOF And create a Service resource, cat > guestbook-service.yaml <<EOF apiVersion: v1 kind: Service metadata: name: guestbook labels: app: guestbook spec: type: LoadBalancer ports: - port: 80 targetPort: 3000 name: http selector: app: guestbook EOF Now, create the Guestbook app using the in-memory database, oc create -f guestbook-deployment.yaml oc create -f guestbook-service.yaml oc expose service guestbook ROUTE = $( oc get route guestbook -o json | jq -r '.spec.host' ) echo $ROUTE Test the application by creating a message and read all current messages, curl -X POST http:// $ROUTE /api/entries -H 'Content-Type: application/json' -H 'Accept: application/json' -d '{ \"message\": \"hello1\" }' curl -X GET http:// $ROUTE /api/entries -H 'Accept: application/json' Right now, if you delete and recreate the Deployment for Guestbook, no persistence is configured.","title":"Deploy the Guestbook app"},{"location":"kubeconfig/2_config_using_env/#add-environment-variables-using-env","text":"The NODE_ENV variable is used to localize configuration of Guestbook. Passing an environment variable NODE_ENV=mongo will make the Loopback framework look for configuration files localized for a mongo environment. The datasources and model-config files for a mongo environment are defined in the files datasources.mongo.json and model-config.mongo.json in the server directory. The datasources.mongo.json is configured as follows using environment variables, { \"mongo\" : { \"name\" : \"mongo\" , \"connector\" : \"mongodb\" , \"host\" : \"${MONGO_HOST}\" , \"port\" : \"${MONGO_PORT}\" , \"database\" : \"${MONGO_DB}\" , \"user\" : \"${MONGO_USER}\" , \"password\" : \"${MONGO_PASS}\" , \"url\" : \"\" } } Create a Deployment resource, which includes the expected environment variables to configure the MongoDB connection in the datasources.mongo.json , cat > guestbook-deployment.yaml <<EOF --- apiVersion: apps/v1 kind: Deployment metadata: name: guestbook namespace: my-vault labels: app: guestbook spec: replicas: 1 selector: matchLabels: app: guestbook template: metadata: labels: app: guestbook spec: serviceAccountName: guestbook containers: - name: app image: $DOCKER_USER/guestbook-nodejs:1.0.0 imagePullPolicy: Always ports: - name: http containerPort: 3000 env: - name: NODE_ENV value: \"mongo\" - name: MONGO_HOST value: \"mongodb\" - name: MONGO_PORT value: \"27017\" - name: MONGO_DB value: \"entries\" - name: MONGO_AUTH_DB value: \"admin\" - name: MONGO_USER value: \"mongoadmin\" - name: MONGO_PASS value: \"m0n90s3cr3t\" EOF Delete and recreate the Guestbook Deployment, oc delete deployment guestbook oc create -f guestbook-deployment.yaml Check the environment variables, oc get pods oc exec guestbook-<instance-id> -- printenv Test the application and create a message and read all messages, curl -X POST http:// $ROUTE /api/entries -H 'Content-Type: application/json' -H 'Accept: application/json' -d '{ \"message\": \"hello1\" }' curl -X GET http:// $ROUTE /api/entries -H 'Accept: application/json' To test the database, delete the deployment and create it again, you should now have persisted your messages, oc delete deployment guestbook oc create -f guestbook-deployment.yaml curl -X GET http:// $ROUTE /api/entries -H 'Accept: application/json'","title":"Add Environment Variables using Env"},{"location":"kubeconfig/3_config_using_configmap/","text":"Lab3: Store Key-Value Pairs using ConfigMap \u00b6 Add a ConfigMap \u00b6 Leaving environment variables in a deployment resource is not recommendable. As a first step, you can move environment variables out of the Deployment to a ConfigMap resource, Create a new ConfigMap, cat > guestbook-configmap.yaml <<EOF apiVersion: v1 kind: ConfigMap metadata: name: guestbook data: NODE_ENV: \"mongo\" MONGO_HOST: \"mongodb\" MONGO_PORT: \"27017\" MONGO_DB: \"entries\" MONGO_AUTH_DB: \"admin\" MONGO_USER: \"mongoadmin\" MONGO_PASS: \"m0n90s3cr3t\" EOF Remove the environment variables in the Deployment resource and add the configMap to the Deployment spec instead, cat > guestbook-deployment.yaml <<EOF --- apiVersion: apps/v1 kind: Deployment metadata: name: guestbook namespace: my-vault labels: app: guestbook spec: replicas: 1 selector: matchLabels: app: guestbook template: metadata: labels: app: guestbook spec: serviceAccountName: guestbook containers: - name: guestbook image: $DOCKER_USER/guestbook-nodejs:1.0.0 imagePullPolicy: Always ports: - name: http containerPort: 3000 envFrom: - configMapRef: name: guestbook EOF Create the ConfigMap and deploy the Guestbook app, oc delete deployment guestbook oc create -f guestbook-configmap.yaml oc create -f guestbook-deployment.yaml ROUTE = $( oc get route guestbook -o json | jq -r '.spec.host' ) echo $ROUTE curl -X GET http:// $ROUTE /api/entries -H 'Accept: application/json'","title":"Lab3. ConfigMap"},{"location":"kubeconfig/3_config_using_configmap/#lab3-store-key-value-pairs-using-configmap","text":"","title":"Lab3: Store Key-Value Pairs using ConfigMap"},{"location":"kubeconfig/3_config_using_configmap/#add-a-configmap","text":"Leaving environment variables in a deployment resource is not recommendable. As a first step, you can move environment variables out of the Deployment to a ConfigMap resource, Create a new ConfigMap, cat > guestbook-configmap.yaml <<EOF apiVersion: v1 kind: ConfigMap metadata: name: guestbook data: NODE_ENV: \"mongo\" MONGO_HOST: \"mongodb\" MONGO_PORT: \"27017\" MONGO_DB: \"entries\" MONGO_AUTH_DB: \"admin\" MONGO_USER: \"mongoadmin\" MONGO_PASS: \"m0n90s3cr3t\" EOF Remove the environment variables in the Deployment resource and add the configMap to the Deployment spec instead, cat > guestbook-deployment.yaml <<EOF --- apiVersion: apps/v1 kind: Deployment metadata: name: guestbook namespace: my-vault labels: app: guestbook spec: replicas: 1 selector: matchLabels: app: guestbook template: metadata: labels: app: guestbook spec: serviceAccountName: guestbook containers: - name: guestbook image: $DOCKER_USER/guestbook-nodejs:1.0.0 imagePullPolicy: Always ports: - name: http containerPort: 3000 envFrom: - configMapRef: name: guestbook EOF Create the ConfigMap and deploy the Guestbook app, oc delete deployment guestbook oc create -f guestbook-configmap.yaml oc create -f guestbook-deployment.yaml ROUTE = $( oc get route guestbook -o json | jq -r '.spec.host' ) echo $ROUTE curl -X GET http:// $ROUTE /api/entries -H 'Accept: application/json'","title":"Add a ConfigMap"},{"location":"kubeconfig/4_config_using_secrets/","text":"Lab4: Store Sensitive Data using Secrets \u00b6 Secrets \u00b6 https://kubernetes.io/docs/concepts/configuration/secret For common environment variables using a ConfigMap is a good solution. But for sensitive information like a password, a token, or a key that need base64 encoding, you can use Secrets . Kubernetes provides several builtin types of Secrets: Kubernetes Secrets are by default stored as unencrypted base64-encoded strings. By default they can be retrieved as plain text by anyone with API access, by anyone with the permission to create a Pod, or anyone with access to Kubernetes' underlying data store, etcd. In order to safely use Secrets, it is recommended you (at a minimum): Enable Encryption at Rest for Secrets. Enable or configure RBAC rules that restrict reading and writing the Secret. A Secret can be used with a Pod in three ways: As a file in a mounted volume. As environment variable. By the kubelet when pulling images for the Pod. Opaque, arbitrary user-defined data kubernetes.io/service-account-token, service account token kubernetes.io/dockercfg, serialized ~/.dockercfg file kubernetes.io/dockerconfigjson, serialized ~/.docker/config.json file kubernetes.io/basic-auth, credentials for basic authentication kubernetes.io/ssh-auth, credentials for SSH authentication kubernetes.io/tls, data for a TLS client or server bootstrap.kubernetes.io/token, bootstrap token data, includes the following keys: token-id, token-secret, description, expiration. The kubectl CLI supports the following three secret commands, docker-registry, Create a secret for use with a Docker registry. This command creates a Secret of type kubernetes.io/dockerconfigjson. generic, Create a secret from a local file, directory or literal value tls, Create a TLS secret Secrets can be mounted as a data volume or exposed as environment variables. To set environment variables, include the env or envFrom field. Add a Secret \u00b6 oc delete configmap guestbook cat > guestbook-configmap.yaml <<EOF apiVersion: v1 kind: ConfigMap metadata: name: guestbook data: NODE_ENV: \"mongo\" MONGO_HOST: \"mongodb\" MONGO_PORT: \"27017\" MONGO_DB: \"entries\" MONGO_AUTH_DB: \"admin\" EOF oc create -f guestbook-configmap.yaml Define a Secret for the username and password for Mongodb, MONGO_USER = mongoadmin MONGO_PASS = m0n90s3cr3t cat > guestbook-mongodb-secret.yaml <<EOF apiVersion: v1 kind: Secret metadata: name: guestbook-mongodb-secret data: MONGO_USER: $MONGO_USER MONGO_PASS: $MONGO_PASS EOF Create the Secret, oc create -f guestbook-mongodb-secret.yaml oc describe secret guestbook-mongodb-secret To create the Secret using the kubectl or oc CLI, MONGO_USER = mongoadmin MONGO_PASS = m0n90s3cr3t oc delete secret guestbook-mongodb-secret oc create secret generic guestbook-mongodb-secret --from-literal = \"MONGO_USER= $MONGO_USER \" --from-literal = \"MONGO_PASS= $MONGO_PASS \" As Volume Mount \u00b6 You can use the secret as a volume mount instead of an environment variables. The Guestbook app is using environment variables to configure the MongoDB connection however. cat > guestbook-deployment.yaml << EOF --- apiVersion: apps/v1 kind: Deployment metadata: name: guestbook namespace: my-vault labels: app: guestbook spec: replicas: 1 selector: matchLabels: app: guestbook template: metadata: labels: app: guestbook spec: serviceAccountName: guestbook containers: - name: guestbook image: $DOCKER_USER/guestbook-nodejs:1.0.0 volumeMounts: - name: secret-volume mountPath: /etc/secret-volume readOnly: true envFrom: - configMapRef: name: guestbook volumes: - name: secret-volume secret: secretName: guestbook-mongodb-secret EOF Delete and redeploy the Guestbook app, oc delete deployment guestbook oc create -f guestbook-deployment.yaml Check that the Secrets are mounted correctly, $ oc get pods NAME READY STATUS RESTARTS AGE guestbook-5c5975496-bmswc 1 /1 Running 0 2m1s mongodb-8444497695-79rf9 1 /1 Running 0 5h55m $ oc exec -it guestbook-5c5975496-bmswc -- bash bash-4.4$ ls -al /etc/secret-volume total 8 drwxrwsrwt. 3 root 1000620000 120 Mar 11 03 :15 . drwxr-xr-x. 1 root root 4096 Mar 11 03 :15 .. drwxr-sr-x. 2 root 1000620000 80 Mar 11 03 :15 ..2021_03_11_03_15_11.771178718 lrwxrwxrwx. 1 root root 31 Mar 11 03 :15 ..data -> ..2021_03_11_03_15_11.771178718 lrwxrwxrwx. 1 root root 17 Mar 11 03 :15 MONGO_PASS -> ..data/MONGO_PASS lrwxrwxrwx. 1 root root 17 Mar 11 03 :15 MONGO_USER -> ..data/MONGO_USER $ echo \" $( cat /etc/secret-volume/MONGO_USER ) \" $ echo \" $( cat /etc/secret-volume/MONGO_PASS ) \" $ exit As Environment Variables \u00b6 Instead of mounting the Secret as volume, the Guestbook app requires to configure the Secrets as environment variables, cat > guestbook-deployment.yaml << EOF --- apiVersion: apps/v1 kind: Deployment metadata: name: guestbook namespace: my-vault labels: app: guestbook spec: replicas: 1 selector: matchLabels: app: guestbook template: metadata: labels: app: guestbook spec: serviceAccountName: guestbook containers: - name: guestbook image: $DOCKER_USER/guestbook-nodejs:1.0.0 env: - name: MONGO_USER valueFrom: secretKeyRef: name: guestbook-mongodb-secret key: MONGO_USER - name: MONGO_PASS valueFrom: secretKeyRef: name: guestbook-mongodb-secret key: MONGO_PASS envFrom: - configMapRef: name: guestbook EOF Delete and redeploy the Guestbook app, oc delete deployment guestbook oc create -f guestbook-deployment.yaml Test the Guestbook app deployment, ROUTE = $( oc get route guestbook -o json | jq -r '.spec.host' ) echo $ROUTE curl -X GET http:// $ROUTE /api/entries -H 'Accept: application/json'","title":"Lab4. Secrets"},{"location":"kubeconfig/4_config_using_secrets/#lab4-store-sensitive-data-using-secrets","text":"","title":"Lab4: Store Sensitive Data using Secrets"},{"location":"kubeconfig/4_config_using_secrets/#secrets","text":"https://kubernetes.io/docs/concepts/configuration/secret For common environment variables using a ConfigMap is a good solution. But for sensitive information like a password, a token, or a key that need base64 encoding, you can use Secrets . Kubernetes provides several builtin types of Secrets: Kubernetes Secrets are by default stored as unencrypted base64-encoded strings. By default they can be retrieved as plain text by anyone with API access, by anyone with the permission to create a Pod, or anyone with access to Kubernetes' underlying data store, etcd. In order to safely use Secrets, it is recommended you (at a minimum): Enable Encryption at Rest for Secrets. Enable or configure RBAC rules that restrict reading and writing the Secret. A Secret can be used with a Pod in three ways: As a file in a mounted volume. As environment variable. By the kubelet when pulling images for the Pod. Opaque, arbitrary user-defined data kubernetes.io/service-account-token, service account token kubernetes.io/dockercfg, serialized ~/.dockercfg file kubernetes.io/dockerconfigjson, serialized ~/.docker/config.json file kubernetes.io/basic-auth, credentials for basic authentication kubernetes.io/ssh-auth, credentials for SSH authentication kubernetes.io/tls, data for a TLS client or server bootstrap.kubernetes.io/token, bootstrap token data, includes the following keys: token-id, token-secret, description, expiration. The kubectl CLI supports the following three secret commands, docker-registry, Create a secret for use with a Docker registry. This command creates a Secret of type kubernetes.io/dockerconfigjson. generic, Create a secret from a local file, directory or literal value tls, Create a TLS secret Secrets can be mounted as a data volume or exposed as environment variables. To set environment variables, include the env or envFrom field.","title":"Secrets"},{"location":"kubeconfig/4_config_using_secrets/#add-a-secret","text":"oc delete configmap guestbook cat > guestbook-configmap.yaml <<EOF apiVersion: v1 kind: ConfigMap metadata: name: guestbook data: NODE_ENV: \"mongo\" MONGO_HOST: \"mongodb\" MONGO_PORT: \"27017\" MONGO_DB: \"entries\" MONGO_AUTH_DB: \"admin\" EOF oc create -f guestbook-configmap.yaml Define a Secret for the username and password for Mongodb, MONGO_USER = mongoadmin MONGO_PASS = m0n90s3cr3t cat > guestbook-mongodb-secret.yaml <<EOF apiVersion: v1 kind: Secret metadata: name: guestbook-mongodb-secret data: MONGO_USER: $MONGO_USER MONGO_PASS: $MONGO_PASS EOF Create the Secret, oc create -f guestbook-mongodb-secret.yaml oc describe secret guestbook-mongodb-secret To create the Secret using the kubectl or oc CLI, MONGO_USER = mongoadmin MONGO_PASS = m0n90s3cr3t oc delete secret guestbook-mongodb-secret oc create secret generic guestbook-mongodb-secret --from-literal = \"MONGO_USER= $MONGO_USER \" --from-literal = \"MONGO_PASS= $MONGO_PASS \"","title":"Add a Secret"},{"location":"kubeconfig/4_config_using_secrets/#as-volume-mount","text":"You can use the secret as a volume mount instead of an environment variables. The Guestbook app is using environment variables to configure the MongoDB connection however. cat > guestbook-deployment.yaml << EOF --- apiVersion: apps/v1 kind: Deployment metadata: name: guestbook namespace: my-vault labels: app: guestbook spec: replicas: 1 selector: matchLabels: app: guestbook template: metadata: labels: app: guestbook spec: serviceAccountName: guestbook containers: - name: guestbook image: $DOCKER_USER/guestbook-nodejs:1.0.0 volumeMounts: - name: secret-volume mountPath: /etc/secret-volume readOnly: true envFrom: - configMapRef: name: guestbook volumes: - name: secret-volume secret: secretName: guestbook-mongodb-secret EOF Delete and redeploy the Guestbook app, oc delete deployment guestbook oc create -f guestbook-deployment.yaml Check that the Secrets are mounted correctly, $ oc get pods NAME READY STATUS RESTARTS AGE guestbook-5c5975496-bmswc 1 /1 Running 0 2m1s mongodb-8444497695-79rf9 1 /1 Running 0 5h55m $ oc exec -it guestbook-5c5975496-bmswc -- bash bash-4.4$ ls -al /etc/secret-volume total 8 drwxrwsrwt. 3 root 1000620000 120 Mar 11 03 :15 . drwxr-xr-x. 1 root root 4096 Mar 11 03 :15 .. drwxr-sr-x. 2 root 1000620000 80 Mar 11 03 :15 ..2021_03_11_03_15_11.771178718 lrwxrwxrwx. 1 root root 31 Mar 11 03 :15 ..data -> ..2021_03_11_03_15_11.771178718 lrwxrwxrwx. 1 root root 17 Mar 11 03 :15 MONGO_PASS -> ..data/MONGO_PASS lrwxrwxrwx. 1 root root 17 Mar 11 03 :15 MONGO_USER -> ..data/MONGO_USER $ echo \" $( cat /etc/secret-volume/MONGO_USER ) \" $ echo \" $( cat /etc/secret-volume/MONGO_PASS ) \" $ exit","title":"As Volume Mount"},{"location":"kubeconfig/4_config_using_secrets/#as-environment-variables","text":"Instead of mounting the Secret as volume, the Guestbook app requires to configure the Secrets as environment variables, cat > guestbook-deployment.yaml << EOF --- apiVersion: apps/v1 kind: Deployment metadata: name: guestbook namespace: my-vault labels: app: guestbook spec: replicas: 1 selector: matchLabels: app: guestbook template: metadata: labels: app: guestbook spec: serviceAccountName: guestbook containers: - name: guestbook image: $DOCKER_USER/guestbook-nodejs:1.0.0 env: - name: MONGO_USER valueFrom: secretKeyRef: name: guestbook-mongodb-secret key: MONGO_USER - name: MONGO_PASS valueFrom: secretKeyRef: name: guestbook-mongodb-secret key: MONGO_PASS envFrom: - configMapRef: name: guestbook EOF Delete and redeploy the Guestbook app, oc delete deployment guestbook oc create -f guestbook-deployment.yaml Test the Guestbook app deployment, ROUTE = $( oc get route guestbook -o json | jq -r '.spec.host' ) echo $ROUTE curl -X GET http:// $ROUTE /api/entries -H 'Accept: application/json'","title":"As Environment Variables"},{"location":"kubeconfig/5_config_private_registry/","text":"Lab5: Pull an Image from a Private Registry \u00b6 Using a Private Registry \u00b6 IBM Container Registry \u00b6 (tbd) Quay.io \u00b6 Create a Secret to authenticate to pull images from private Quay.io registry, oc create secret docker-registry quaycreds --docker-server = quay.io --docker-username = remkohdev --docker-password = Vitamin@2 --docker-email = remkohdev@gmail.com -n my-vault Login to the registry, and build, tag and push your image, tbd In your deployment you can add the fully qualified domain to your image on Quay and add the imagePullSecrets property. echo '--- apiVersion: apps/v1 kind: Deployment metadata: name: guestbook namespace: my-vault labels: app: guestbook spec: replicas: 1 selector: matchLabels: app: guestbook template: metadata: labels: app: guestbook spec: serviceAccountName: guestbook containers: - name: guestbook image: quay.io/remkohdev/guestbook-nodejs:1.0.0 imagePullPolicy: Always ports: - name: http containerPort: 3000 env: - name: MONGO_USER valueFrom: secretKeyRef: name: guestbook-mongodb-secret key: MONGO_USER - name: MONGO_PASS valueFrom: secretKeyRef: name: guestbook-mongodb-secret key: MONGO_PASS envFrom: - configMapRef: name: guestbook imagePullSecrets: - name: quaycreds' > guestbook-deployment.yaml (tbd: use ImageStream on OpenShift to automatically detect new images)","title":"Lab5. Config Private Registry"},{"location":"kubeconfig/5_config_private_registry/#lab5-pull-an-image-from-a-private-registry","text":"","title":"Lab5: Pull an Image from a Private Registry"},{"location":"kubeconfig/5_config_private_registry/#using-a-private-registry","text":"","title":"Using a Private Registry"},{"location":"kubeconfig/5_config_private_registry/#ibm-container-registry","text":"(tbd)","title":"IBM Container Registry"},{"location":"kubeconfig/5_config_private_registry/#quayio","text":"Create a Secret to authenticate to pull images from private Quay.io registry, oc create secret docker-registry quaycreds --docker-server = quay.io --docker-username = remkohdev --docker-password = Vitamin@2 --docker-email = remkohdev@gmail.com -n my-vault Login to the registry, and build, tag and push your image, tbd In your deployment you can add the fully qualified domain to your image on Quay and add the imagePullSecrets property. echo '--- apiVersion: apps/v1 kind: Deployment metadata: name: guestbook namespace: my-vault labels: app: guestbook spec: replicas: 1 selector: matchLabels: app: guestbook template: metadata: labels: app: guestbook spec: serviceAccountName: guestbook containers: - name: guestbook image: quay.io/remkohdev/guestbook-nodejs:1.0.0 imagePullPolicy: Always ports: - name: http containerPort: 3000 env: - name: MONGO_USER valueFrom: secretKeyRef: name: guestbook-mongodb-secret key: MONGO_USER - name: MONGO_PASS valueFrom: secretKeyRef: name: guestbook-mongodb-secret key: MONGO_PASS envFrom: - configMapRef: name: guestbook imagePullSecrets: - name: quaycreds' > guestbook-deployment.yaml (tbd: use ImageStream on OpenShift to automatically detect new images)","title":"Quay.io"},{"location":"kubeconfig/6_encryption_for_config/","text":"","title":"6 encryption for config"},{"location":"vault/","text":"Vault \u00b6 Lab1. Manage Encrypted Secrets with IBM Secrets Manager (Managed Vault) Lab2. Setup Internal Vault with Vault Agent Injector on OpenShift Lab3. Access Internal Vault using Vault Agent Injector","title":"Vault"},{"location":"vault/#vault","text":"Lab1. Manage Encrypted Secrets with IBM Secrets Manager (Managed Vault) Lab2. Setup Internal Vault with Vault Agent Injector on OpenShift Lab3. Access Internal Vault using Vault Agent Injector","title":"Vault"},{"location":"vault/1_ibm_secrets_manager/","text":"Lab1: Manage Encrypted Secrets with IBM Secrets Manager (Managed Vault) \u00b6 Suppose you have an application guestbook , which persists data to a MongoDB instance. To access MongoDB, guestbook needs a username and password for MongoDB. Instead of configuring credentials as code64 encoded secrets, we store encrypted credentials in Vault. Using the Vault Agent Injector the app retrieves the encrypted secrets from Vault using Kubernetes authentication and an in-memory volume mount. In this lab we will create and use a free instance of IBM Secrets Manager with Lite plan. The free Lite plan is limited to 1 instance per account. IBM Secrets Manager is a cloud managed instance of Vault. Steps: Create an IBM Secrets Manager Instance, Create an IBM Cloud API Key, Create an Access Token, Using the Vault CLI, Create a Secret Group, Static versus Dynamic Secrets, Create a Static Secret, Read Secret, Make sure you are logged in to your IBM Cloud account, ibmcloud login [ -sso ] Check for existing resources of IBM Secrets Manager, ibmcloud resource service-instances If you already have a Lite instance of IBM Secrets Manager, you can skip step 1 to create a new instance, or you can delete the existing instance as follows using the name or id of the service, SERVICE_NAME = remkohdev-vault-1 SERVICE_ID = crn:v1:bluemix:public:secrets-manager:us-south:a/3fe3c0de197257ef62d81c9f9c0f33aa:5c9c312c-1d3f-4cc7-839b-a95f7305896f:: ibmcloud resource service-instance-delete $SERVICE_NAME Create an IBM Secrets Manager Instance \u00b6 Make sure the secrets-manager plugin for IBM Cloud CLI is installed, $ ibmcloud plugin list Listing installed plug-ins... Plugin Name Version Status Private endpoints supported cloud-object-storage 1 .2.2 false machine-learning 3 .0.2 false analytics-engine 1 .0.166 false code-engine/ce 0 .5.20 false container-registry 0 .1.514 false power-iaas 0 .3.1 false cloud-databases 0 .10.2 false cloud-dns-services 0 .3.4 false cloud-functions/wsk/functions/fn 1 .0.49 false cloud-internet-services 1 .13.1 false push-notifications 1 .0.3 false app-configuration 0 .0.1 false dbaas-cli 1 .7.1 false doi 0 .3.1 false secrets-manager 0 .0.6 false tg-cli/tg 0 .3.0 false schematics 1 .5.1 false watson 0 .0.9 false catalogs-management 1 .0.6 true hpvs 1 .4.2 false observe-service/ob 1 .0.61 false tke 1 .1.4 false activity-tracker 3 .3.4 false auto-scaling 0 .2.8 false container-service/kubernetes-service 1 .0.233 false dl-cli 0 .3.0 false event-streams 2 .3.0 false key-protect 0 .6.0 false vpc-infrastructure/infrastructure-service 0 .7.8 Update Available false whcs 0 .0.5 false If the secrets-manager plugin is not installed, install it, ibmcloud plugin install secrets-manager With the IBM Cloud CLI and secrets-manager plugin installed, create a free Lite instance of IBM Secrets Manager with name set in the environment variable SM_NAME , SM_NAME = remkohdev-vault-1 SM_PLAN = lite REGION = us-south RG = default ibmcloud resource service-instance-create $SM_NAME secrets-manager $SM_PLAN $REGION -g $RG To view existing resource groups, $ ibmcloud resource groups Retrieving all resource groups under account 3fe3c0de197257ef62d81c9f9c0f33aa as remkohdev@us.ibm.com... OK Name ID Default Group State default 46ca8ccd59ae4f97b2a6b84539342d1c true ACTIVE To view available regions, $ ibmcloud regions Listing regions... Name Display name au-syd Sydney in -che Chennai jp-osa Osaka jp-tok Tokyo kr-seo Seoul eu-de Frankfurt eu-gb London ca-tor Toronto us-south Dallas us-south-test Dallas Test us-east Washington DC br-sao Sao Paolo Note that not all services might be available in all regions. For this lab, I used the us-south region. $ ibmcloud resource service-instance $SM_NAME Retrieving service instance remkohdev-vault-1 in all resource groups under account REMKO DE KNIKKER ' s Account as remkohdev@us.ibm.com... OK Name: remkohdev-vault-1 ID: crn:v1:bluemix:public:secrets-manager:us-south:a/3fe3c0de197257ef62d81c9f9c0f33aa:5c9c312c-1d3f-4cc7-839b-a95f7305896f:: GUID: 5c9c312c-1d3f-4cc7-839b-a95f7305896f Location: us-south Service Name: secrets-manager Service Plan Name: lite Resource Group Name: default State: active Type: service_instance Sub Type: Created at: 2021 -02-25T15:41:12Z Created by: remkohdev@us.ibm.com Updated at: 2021 -02-25T15:46:30Z Last Operation: Status create succeeded Message Task not found Get the GUID of the Secrets Manager instance, SM_GUID = $( ibmcloud resource service-instance $SM_NAME --output json | jq -r '.[0].guid' ) echo $SM_GUID Construct the Secrets Manager API URL, SM_API_URL = https:// $SM_GUID . $REGION .secrets-manager.appdomain.cloud echo $SM_API_URL Create an IBM Cloud API Key \u00b6 IAM_APIKEY_NAME = $SM_NAME -apikey1 ibmcloud iam api-key-create $IAM_APIKEY_NAME -d \"API key for Secrets Manager\" --file $IAM_APIKEY_NAME .txt IAM_APIKEY = $( cat $IAM_APIKEY_NAME .txt | jq -r '.apikey' ) echo $IAM_APIKEY Create an Access Token \u00b6 Using the IAM_APIKEY value for the IBM Cloud API Key, create and retrieve an access token, IAM_TOKEN = $( curl -X POST \\ \"https://iam.cloud.ibm.com/identity/token\" \\ --header 'Content-Type: application/x-www-form-urlencoded' \\ --header 'Accept: application/json' \\ --data-urlencode 'grant_type=urn:ibm:params:oauth:grant-type:apikey' \\ --data-urlencode apikey = $IAM_APIKEY | jq -r '.access_token' ) Check if the access token was retrieved successfully, $ echo $IAM_TOKEN { \"access_token\" : \"eyJraWQiOiIyMDIxMDIxOTE4MzUiLCJhbGciOiJSUzI1NiJ9.eyJpYW1faWQiOiJJQk1pZC0zMTAwMDBGQ1A1IiwiaWQiOiJJQk1pZC0zMTAwMDBGQ1A1IiwicmVhbG1pZCI6IklCTWlkIiwianRpIjoiM2UzYjk5ODItN2Q5NC00NDVlLTk5OWQtMzgyOTQ4ZjE2ZmI4IiwiaWRlbnRpZmllciI6IjMxMDAwMEZDUDUiLCJnaXZlbl9uYW1lIjoiUkVNS08iLCJmYW1pbHlfbmFtZSI6IkRFIEtOSUtLRVIiLCJuYW1lIjoiUkVNS08gREUgS05JS0tFUiIsImVtYWlsIjoicmVta29oZGV2QHVzLmlibS5jb20iLCJzdWIiOiJyZW1rb2hkZXZAdXMuaWJtLmNvbSIsImF1dGhuIjp7InN1YiI6InJlbWtvaGRldkB1cy5pYm0uY29tIiwiaWFtX2lkIjoiaWFtLTMxMDAwMEZDUDUiLCJuYW1lIjoiUkVNS08gREUgS05JS0tFUiIsImdpdmVuX25hbWUiOiJSRU1LTyIsImZhbWlseV9uYW1lIjoiREUgS05JS0tFUiIsImVtYWlsIjoicmVta29oZGV2QHVzLmlibS5jb20ifSwiYWNjb3VudCI6eyJ2YWxpZCI6dHJ1ZSwiYnNzIjoiM2ZlM2MwZGUxOTcyNTdlZjYyZDgxYzlmOWMwZjMzYWEiLCJpbXNfdXNlcl9pZCI6IjYzOTE4NjMiLCJmcm96ZW4iOnRydWUsImltcyI6IjEwNTc2MDcifSwiaWF0IjoxNjE0MjY5NzU5LCJleHAiOjE2MTQyNzMzNTksImlzcyI6Imh0dHBzOi8vaWFtLmNsb3VkLmlibS5jb20vaWRlbnRpdHkiLCJncmFudF90eXBlIjoidXJuOmlibTpwYXJhbXM6b2F1dGg6Z3JhbnQtdHlwZTphcGlrZXkiLCJzY29wZSI6ImlibSBvcGVuaWQiLCJjbGllbnRfaWQiOiJkZWZhdWx0IiwiYWNyIjoxLCJhbXIiOlsicHdkIl19.Mx7jUxjnO8XFW0ZrnByYiUeU6dI7PnbRDEBRObx7H8RUMjkk4lQUbYxjGTgg6QficBKBjNVIhJ4dtzupqtkMy_cyXmgjMiUEmEAV4gTEaljcmFvJb27EvHhWUAv7uSOOQRLL_kByj_EcjohRB6Jwxdx3XNZbVkGgZ8sSnclrtWLGG35KKP5xghBUNYChfe-lbnQh_i5qszGsjJAa1uQ5zqjFRyRKwDkTR-sfOhBRh2legJOCeWsT5kxbtNw4xebOAJQHUQB087QElYJfMOkPpOyX7KMVvuctKGR--z_Rb5_lUyt_JSZTFDFR4SVfHrHA-5fhXv0Ykqv5HBRrd4yTgw\" , \"refresh_token\" : \"not_supported\" , \"ims_user_id\" :6391863, \"token_type\" : \"Bearer\" , \"expires_in\" :3600, \"expiration\" :1614273359, \"scope\" : \"ibm openid\" } Using the Vault CLI \u00b6 If you installed the vault cli you can alternatively use the Vault CLI instead of the API. export VAULT_ADDR = $SM_API_URL vault write -format = json auth/ibmcloud/login token = $IAM_TOKEN vault secrets enable -path = secret/myapp/config kv vault kv put secret/myapp/config username = $MY_USERNAME password = $MY_PASSWORD For more details, see the documentation for Vault Commands (CLI) . Create a Secret Group \u00b6 Secrets are organized in secret groups. Define the following bash environment variables, MY_APP = guestbook SECRET_GROUP = $MY_APP -secretgroup1 SECRET_GROUP_DESCRIPTION = \"secret group 1 for application $MY_APP \" echo $SECRET_GROUP_DESCRIPTION Create a secret group, $ curl -X POST \" $SM_API_URL /api/v1/secret_groups\" -H \"Authorization: Bearer $IAM_TOKEN \" -H \"Accept: application/json\" -H \"Content-Type: application/json\" -d '{\"metadata\": {\"collection_type\": \"application/vnd.ibm.secrets-manager.secret.group+json\", \"collection_total\": 1 }, \"resources\": [{ \"name\": \"' \" $SECRET_GROUP \" '\", \"description\": \"' \" $SECRET_GROUP_DESCRIPTION \" '\" }]}' { \"metadata\" : { \"collection_type\" : \"application/vnd.ibm.secrets-manager.secret.group+json\" , \"collection_total\" :1 } , \"resources\" : [{ \"creation_date\" : \"2021-03-04T19:18:46Z\" , \"description\" : \"secret group 1 for applicati on guestbook\" , \"id\" : \"3a225644-8083-1ae6-2ef5-d3972c046899\" , \"last_update_date\" : \"2021-03-04T19:18:46Z\" , \"name\" : \"guestbook-secretgroup1\" , \"type\" : \"application/vnd.ibm.secrets-manager.secret.group+json\" }]} $ SECRET_GROUP_ID = $( curl -X GET \" $SM_API_URL /api/v1/secret_groups\" -H \"Authorization: Bearer $IAM_TOKEN \" -H \"Accept: application/json\" | jq -r '.resources[0].id' ) $ echo $SECRET_GROUP_ID 3a225644-8083-1ae6-2ef5-d3972c046899 Static versus Dynamic Secrets \u00b6 In this tutorial, I use static secrets using a key-value pair. Dynamic secrets can be generated for some systems on-demand by Vault. For when an app wants to access S3 storage like IBM Cloud Object Storage (COS), it asks Vault for credentials. Vault then generates COS credentials granting permission to accss the S3 bucket. If you want to use a dynamic secret to access a Cloud Object Storage instead see this tutorial here . Create a Static Secret \u00b6 Use the following environment variables to create a secret, USERNAME = remkohdev PASSWORD = Passw0rd1 SECRET_NAME = \"mongodb-dev-creds-secret1\" SECRET_DESCRIPTION = \"username password credentials for Mongodb instance in dev environment\" EXP_DATE = \"2021-12-31T00:00:00Z\" DEPLOYMENT_ENV = dev DEPLOYMENT_REGION = us-south Create a static secret, curl -X POST \" $SM_API_URL /api/v1/secrets/username_password\" -H \"Authorization: Bearer $IAM_TOKEN \" -H \"Accept: application/json\" -H \"Content-Type: application/json\" -d '{\"metadata\": { \"collection_type\": \"application/vnd.ibm.secrets-manager.secret+json\", \"collection_total\": 1 }, \"resources\": [{ \"name\": \"' \" $SECRET_NAME \" '\", \"description\": \"' \" $SECRET_DESCRIPTION \" '\", \"secret_group_id\": \"' \" $SECRET_GROUP_ID \" '\", \"username\": \"' \" $USERNAME \" '\", \"password\": \"' \" $PASSWORD \" '\", \"expiration_date\": \"' \" $EXP_DATE \" '\", \"labels\": [ \"' \" $DEPLOYMENT_ENV \" '\", \"' \" $DEPLOYMENT_REGION \" '\" ]}]}' Read Secret \u00b6 Read a username_password type secret using the API, $ curl -X GET \" $SM_API_URL /api/v1/secrets\" -H \"Authorization: Bearer $IAM_TOKEN \" -H \"Accept: application/json\" { \"metadata\" : { \"collection_type\" : \"application/vnd.ibm.secrets-manager.secret+json\" , \"collection_total\" : 1 } , \"resources\" : [ { \"created_by\" : \"IBMid-310000FCP5\" , \"creation_date\" : \"2021-03-04T19:43:00Z\" , \"crn\" : \"crn:v1:bluemix:public:secrets-manager:us-south:a/3fe3c0de197257ef62d81c9f9c0f33aa:c84703a7-c37e-490f-ab8a-e24ddbcc0bcc:secret:5823ca61-20b9-75f4-804d-0d85a9fc58be\" , \"description\" : \"username password credentials for Mongodb instance in dev environment\" , \"expiration_date\" : \"2021-12-31T00:00:00Z\" , \"id\" : \"5823ca61-20b9-75f4-804d-0d85a9fc58be\" , \"labels\" : [ \"dev\" , \"us-south\" ] , \"last_update_date\" : \"2021-03-04T19:43:00Z\" , \"name\" : \"mongodb-dev-creds-secret1\" , \"secret_group_id\" : \"3a225644-8083-1ae6-2ef5-d3972c046899\" , \"secret_type\" : \"username_password\" , \"state\" : 1 , \"state_description\" : \"Active\" } ] } Conclusion \u00b6 You're awesome! You stored database credentials in an encrypted secrets engine using Vault. A next step could be to inject the credentials into a pod running on Kubernetes or OpenShift to securely connect to a MongoDB database. Or you can explore using dynamic secrets with an S3 object storage like IBM Cloud Object Storage.","title":"Lab1. IBM Secrets Manager"},{"location":"vault/1_ibm_secrets_manager/#lab1-manage-encrypted-secrets-with-ibm-secrets-manager-managed-vault","text":"Suppose you have an application guestbook , which persists data to a MongoDB instance. To access MongoDB, guestbook needs a username and password for MongoDB. Instead of configuring credentials as code64 encoded secrets, we store encrypted credentials in Vault. Using the Vault Agent Injector the app retrieves the encrypted secrets from Vault using Kubernetes authentication and an in-memory volume mount. In this lab we will create and use a free instance of IBM Secrets Manager with Lite plan. The free Lite plan is limited to 1 instance per account. IBM Secrets Manager is a cloud managed instance of Vault. Steps: Create an IBM Secrets Manager Instance, Create an IBM Cloud API Key, Create an Access Token, Using the Vault CLI, Create a Secret Group, Static versus Dynamic Secrets, Create a Static Secret, Read Secret, Make sure you are logged in to your IBM Cloud account, ibmcloud login [ -sso ] Check for existing resources of IBM Secrets Manager, ibmcloud resource service-instances If you already have a Lite instance of IBM Secrets Manager, you can skip step 1 to create a new instance, or you can delete the existing instance as follows using the name or id of the service, SERVICE_NAME = remkohdev-vault-1 SERVICE_ID = crn:v1:bluemix:public:secrets-manager:us-south:a/3fe3c0de197257ef62d81c9f9c0f33aa:5c9c312c-1d3f-4cc7-839b-a95f7305896f:: ibmcloud resource service-instance-delete $SERVICE_NAME","title":"Lab1: Manage Encrypted Secrets with IBM Secrets Manager (Managed Vault)"},{"location":"vault/1_ibm_secrets_manager/#create-an-ibm-secrets-manager-instance","text":"Make sure the secrets-manager plugin for IBM Cloud CLI is installed, $ ibmcloud plugin list Listing installed plug-ins... Plugin Name Version Status Private endpoints supported cloud-object-storage 1 .2.2 false machine-learning 3 .0.2 false analytics-engine 1 .0.166 false code-engine/ce 0 .5.20 false container-registry 0 .1.514 false power-iaas 0 .3.1 false cloud-databases 0 .10.2 false cloud-dns-services 0 .3.4 false cloud-functions/wsk/functions/fn 1 .0.49 false cloud-internet-services 1 .13.1 false push-notifications 1 .0.3 false app-configuration 0 .0.1 false dbaas-cli 1 .7.1 false doi 0 .3.1 false secrets-manager 0 .0.6 false tg-cli/tg 0 .3.0 false schematics 1 .5.1 false watson 0 .0.9 false catalogs-management 1 .0.6 true hpvs 1 .4.2 false observe-service/ob 1 .0.61 false tke 1 .1.4 false activity-tracker 3 .3.4 false auto-scaling 0 .2.8 false container-service/kubernetes-service 1 .0.233 false dl-cli 0 .3.0 false event-streams 2 .3.0 false key-protect 0 .6.0 false vpc-infrastructure/infrastructure-service 0 .7.8 Update Available false whcs 0 .0.5 false If the secrets-manager plugin is not installed, install it, ibmcloud plugin install secrets-manager With the IBM Cloud CLI and secrets-manager plugin installed, create a free Lite instance of IBM Secrets Manager with name set in the environment variable SM_NAME , SM_NAME = remkohdev-vault-1 SM_PLAN = lite REGION = us-south RG = default ibmcloud resource service-instance-create $SM_NAME secrets-manager $SM_PLAN $REGION -g $RG To view existing resource groups, $ ibmcloud resource groups Retrieving all resource groups under account 3fe3c0de197257ef62d81c9f9c0f33aa as remkohdev@us.ibm.com... OK Name ID Default Group State default 46ca8ccd59ae4f97b2a6b84539342d1c true ACTIVE To view available regions, $ ibmcloud regions Listing regions... Name Display name au-syd Sydney in -che Chennai jp-osa Osaka jp-tok Tokyo kr-seo Seoul eu-de Frankfurt eu-gb London ca-tor Toronto us-south Dallas us-south-test Dallas Test us-east Washington DC br-sao Sao Paolo Note that not all services might be available in all regions. For this lab, I used the us-south region. $ ibmcloud resource service-instance $SM_NAME Retrieving service instance remkohdev-vault-1 in all resource groups under account REMKO DE KNIKKER ' s Account as remkohdev@us.ibm.com... OK Name: remkohdev-vault-1 ID: crn:v1:bluemix:public:secrets-manager:us-south:a/3fe3c0de197257ef62d81c9f9c0f33aa:5c9c312c-1d3f-4cc7-839b-a95f7305896f:: GUID: 5c9c312c-1d3f-4cc7-839b-a95f7305896f Location: us-south Service Name: secrets-manager Service Plan Name: lite Resource Group Name: default State: active Type: service_instance Sub Type: Created at: 2021 -02-25T15:41:12Z Created by: remkohdev@us.ibm.com Updated at: 2021 -02-25T15:46:30Z Last Operation: Status create succeeded Message Task not found Get the GUID of the Secrets Manager instance, SM_GUID = $( ibmcloud resource service-instance $SM_NAME --output json | jq -r '.[0].guid' ) echo $SM_GUID Construct the Secrets Manager API URL, SM_API_URL = https:// $SM_GUID . $REGION .secrets-manager.appdomain.cloud echo $SM_API_URL","title":"Create an IBM Secrets Manager Instance"},{"location":"vault/1_ibm_secrets_manager/#create-an-ibm-cloud-api-key","text":"IAM_APIKEY_NAME = $SM_NAME -apikey1 ibmcloud iam api-key-create $IAM_APIKEY_NAME -d \"API key for Secrets Manager\" --file $IAM_APIKEY_NAME .txt IAM_APIKEY = $( cat $IAM_APIKEY_NAME .txt | jq -r '.apikey' ) echo $IAM_APIKEY","title":"Create an IBM Cloud API Key"},{"location":"vault/1_ibm_secrets_manager/#create-an-access-token","text":"Using the IAM_APIKEY value for the IBM Cloud API Key, create and retrieve an access token, IAM_TOKEN = $( curl -X POST \\ \"https://iam.cloud.ibm.com/identity/token\" \\ --header 'Content-Type: application/x-www-form-urlencoded' \\ --header 'Accept: application/json' \\ --data-urlencode 'grant_type=urn:ibm:params:oauth:grant-type:apikey' \\ --data-urlencode apikey = $IAM_APIKEY | jq -r '.access_token' ) Check if the access token was retrieved successfully, $ echo $IAM_TOKEN { \"access_token\" : \"eyJraWQiOiIyMDIxMDIxOTE4MzUiLCJhbGciOiJSUzI1NiJ9.eyJpYW1faWQiOiJJQk1pZC0zMTAwMDBGQ1A1IiwiaWQiOiJJQk1pZC0zMTAwMDBGQ1A1IiwicmVhbG1pZCI6IklCTWlkIiwianRpIjoiM2UzYjk5ODItN2Q5NC00NDVlLTk5OWQtMzgyOTQ4ZjE2ZmI4IiwiaWRlbnRpZmllciI6IjMxMDAwMEZDUDUiLCJnaXZlbl9uYW1lIjoiUkVNS08iLCJmYW1pbHlfbmFtZSI6IkRFIEtOSUtLRVIiLCJuYW1lIjoiUkVNS08gREUgS05JS0tFUiIsImVtYWlsIjoicmVta29oZGV2QHVzLmlibS5jb20iLCJzdWIiOiJyZW1rb2hkZXZAdXMuaWJtLmNvbSIsImF1dGhuIjp7InN1YiI6InJlbWtvaGRldkB1cy5pYm0uY29tIiwiaWFtX2lkIjoiaWFtLTMxMDAwMEZDUDUiLCJuYW1lIjoiUkVNS08gREUgS05JS0tFUiIsImdpdmVuX25hbWUiOiJSRU1LTyIsImZhbWlseV9uYW1lIjoiREUgS05JS0tFUiIsImVtYWlsIjoicmVta29oZGV2QHVzLmlibS5jb20ifSwiYWNjb3VudCI6eyJ2YWxpZCI6dHJ1ZSwiYnNzIjoiM2ZlM2MwZGUxOTcyNTdlZjYyZDgxYzlmOWMwZjMzYWEiLCJpbXNfdXNlcl9pZCI6IjYzOTE4NjMiLCJmcm96ZW4iOnRydWUsImltcyI6IjEwNTc2MDcifSwiaWF0IjoxNjE0MjY5NzU5LCJleHAiOjE2MTQyNzMzNTksImlzcyI6Imh0dHBzOi8vaWFtLmNsb3VkLmlibS5jb20vaWRlbnRpdHkiLCJncmFudF90eXBlIjoidXJuOmlibTpwYXJhbXM6b2F1dGg6Z3JhbnQtdHlwZTphcGlrZXkiLCJzY29wZSI6ImlibSBvcGVuaWQiLCJjbGllbnRfaWQiOiJkZWZhdWx0IiwiYWNyIjoxLCJhbXIiOlsicHdkIl19.Mx7jUxjnO8XFW0ZrnByYiUeU6dI7PnbRDEBRObx7H8RUMjkk4lQUbYxjGTgg6QficBKBjNVIhJ4dtzupqtkMy_cyXmgjMiUEmEAV4gTEaljcmFvJb27EvHhWUAv7uSOOQRLL_kByj_EcjohRB6Jwxdx3XNZbVkGgZ8sSnclrtWLGG35KKP5xghBUNYChfe-lbnQh_i5qszGsjJAa1uQ5zqjFRyRKwDkTR-sfOhBRh2legJOCeWsT5kxbtNw4xebOAJQHUQB087QElYJfMOkPpOyX7KMVvuctKGR--z_Rb5_lUyt_JSZTFDFR4SVfHrHA-5fhXv0Ykqv5HBRrd4yTgw\" , \"refresh_token\" : \"not_supported\" , \"ims_user_id\" :6391863, \"token_type\" : \"Bearer\" , \"expires_in\" :3600, \"expiration\" :1614273359, \"scope\" : \"ibm openid\" }","title":"Create an Access Token"},{"location":"vault/1_ibm_secrets_manager/#using-the-vault-cli","text":"If you installed the vault cli you can alternatively use the Vault CLI instead of the API. export VAULT_ADDR = $SM_API_URL vault write -format = json auth/ibmcloud/login token = $IAM_TOKEN vault secrets enable -path = secret/myapp/config kv vault kv put secret/myapp/config username = $MY_USERNAME password = $MY_PASSWORD For more details, see the documentation for Vault Commands (CLI) .","title":"Using the Vault CLI"},{"location":"vault/1_ibm_secrets_manager/#create-a-secret-group","text":"Secrets are organized in secret groups. Define the following bash environment variables, MY_APP = guestbook SECRET_GROUP = $MY_APP -secretgroup1 SECRET_GROUP_DESCRIPTION = \"secret group 1 for application $MY_APP \" echo $SECRET_GROUP_DESCRIPTION Create a secret group, $ curl -X POST \" $SM_API_URL /api/v1/secret_groups\" -H \"Authorization: Bearer $IAM_TOKEN \" -H \"Accept: application/json\" -H \"Content-Type: application/json\" -d '{\"metadata\": {\"collection_type\": \"application/vnd.ibm.secrets-manager.secret.group+json\", \"collection_total\": 1 }, \"resources\": [{ \"name\": \"' \" $SECRET_GROUP \" '\", \"description\": \"' \" $SECRET_GROUP_DESCRIPTION \" '\" }]}' { \"metadata\" : { \"collection_type\" : \"application/vnd.ibm.secrets-manager.secret.group+json\" , \"collection_total\" :1 } , \"resources\" : [{ \"creation_date\" : \"2021-03-04T19:18:46Z\" , \"description\" : \"secret group 1 for applicati on guestbook\" , \"id\" : \"3a225644-8083-1ae6-2ef5-d3972c046899\" , \"last_update_date\" : \"2021-03-04T19:18:46Z\" , \"name\" : \"guestbook-secretgroup1\" , \"type\" : \"application/vnd.ibm.secrets-manager.secret.group+json\" }]} $ SECRET_GROUP_ID = $( curl -X GET \" $SM_API_URL /api/v1/secret_groups\" -H \"Authorization: Bearer $IAM_TOKEN \" -H \"Accept: application/json\" | jq -r '.resources[0].id' ) $ echo $SECRET_GROUP_ID 3a225644-8083-1ae6-2ef5-d3972c046899","title":"Create a Secret Group"},{"location":"vault/1_ibm_secrets_manager/#static-versus-dynamic-secrets","text":"In this tutorial, I use static secrets using a key-value pair. Dynamic secrets can be generated for some systems on-demand by Vault. For when an app wants to access S3 storage like IBM Cloud Object Storage (COS), it asks Vault for credentials. Vault then generates COS credentials granting permission to accss the S3 bucket. If you want to use a dynamic secret to access a Cloud Object Storage instead see this tutorial here .","title":"Static versus Dynamic Secrets"},{"location":"vault/1_ibm_secrets_manager/#create-a-static-secret","text":"Use the following environment variables to create a secret, USERNAME = remkohdev PASSWORD = Passw0rd1 SECRET_NAME = \"mongodb-dev-creds-secret1\" SECRET_DESCRIPTION = \"username password credentials for Mongodb instance in dev environment\" EXP_DATE = \"2021-12-31T00:00:00Z\" DEPLOYMENT_ENV = dev DEPLOYMENT_REGION = us-south Create a static secret, curl -X POST \" $SM_API_URL /api/v1/secrets/username_password\" -H \"Authorization: Bearer $IAM_TOKEN \" -H \"Accept: application/json\" -H \"Content-Type: application/json\" -d '{\"metadata\": { \"collection_type\": \"application/vnd.ibm.secrets-manager.secret+json\", \"collection_total\": 1 }, \"resources\": [{ \"name\": \"' \" $SECRET_NAME \" '\", \"description\": \"' \" $SECRET_DESCRIPTION \" '\", \"secret_group_id\": \"' \" $SECRET_GROUP_ID \" '\", \"username\": \"' \" $USERNAME \" '\", \"password\": \"' \" $PASSWORD \" '\", \"expiration_date\": \"' \" $EXP_DATE \" '\", \"labels\": [ \"' \" $DEPLOYMENT_ENV \" '\", \"' \" $DEPLOYMENT_REGION \" '\" ]}]}'","title":"Create a Static Secret"},{"location":"vault/1_ibm_secrets_manager/#read-secret","text":"Read a username_password type secret using the API, $ curl -X GET \" $SM_API_URL /api/v1/secrets\" -H \"Authorization: Bearer $IAM_TOKEN \" -H \"Accept: application/json\" { \"metadata\" : { \"collection_type\" : \"application/vnd.ibm.secrets-manager.secret+json\" , \"collection_total\" : 1 } , \"resources\" : [ { \"created_by\" : \"IBMid-310000FCP5\" , \"creation_date\" : \"2021-03-04T19:43:00Z\" , \"crn\" : \"crn:v1:bluemix:public:secrets-manager:us-south:a/3fe3c0de197257ef62d81c9f9c0f33aa:c84703a7-c37e-490f-ab8a-e24ddbcc0bcc:secret:5823ca61-20b9-75f4-804d-0d85a9fc58be\" , \"description\" : \"username password credentials for Mongodb instance in dev environment\" , \"expiration_date\" : \"2021-12-31T00:00:00Z\" , \"id\" : \"5823ca61-20b9-75f4-804d-0d85a9fc58be\" , \"labels\" : [ \"dev\" , \"us-south\" ] , \"last_update_date\" : \"2021-03-04T19:43:00Z\" , \"name\" : \"mongodb-dev-creds-secret1\" , \"secret_group_id\" : \"3a225644-8083-1ae6-2ef5-d3972c046899\" , \"secret_type\" : \"username_password\" , \"state\" : 1 , \"state_description\" : \"Active\" } ] }","title":"Read Secret"},{"location":"vault/1_ibm_secrets_manager/#conclusion","text":"You're awesome! You stored database credentials in an encrypted secrets engine using Vault. A next step could be to inject the credentials into a pod running on Kubernetes or OpenShift to securely connect to a MongoDB database. Or you can explore using dynamic secrets with an S3 object storage like IBM Cloud Object Storage.","title":"Conclusion"},{"location":"vault/2_setup_internal_vault/","text":"Lab2: Setup Internal Vault with Agent Injector on OpenShift \u00b6 Setup Helm v3 \u00b6 In the IBM Cloud shell create an alias for Helm v3, alias helm = helm3 Connect to OpenShift \u00b6 oc login --token = <your-openshift-token> --server = <your-openshift-cluster-url> Create a new project or namespace to install Vault, VAULT_NAMESPACE = my-vault-0 oc new-project $VAULT_NAMESPACE Install Vault using Helm Chart \u00b6 The recommended way to run Vault on OpenShift is using the Helm Chart. helm repo add hashicorp https://helm.releases.hashicorp.com Create a custom values.yaml file to set custom defaults for the Vault Helm chart. The full default values.yaml for Vault Helm chart is found at hashicorp/vault-helm . In the postStart hook, in Vault enable Kubernetes authentication and configure Vault to securely communicate with OpenShift. echo '# Custom values for the Vault chart global: # If deploying to OpenShift openshift: true server: dev: enabled: true serviceAccount: create: true name: vault-sa injector: enabled: true authDelegator: enabled: true' > my_values.yaml By default, the injector.enabled parameter is set to true . So together with Vault, the Helm chart installed a Vault Agent Injector Admission Webhook Controller in Kubernetes. By default, the global.openshift parameter is set to false . If set to true , it enables configuration specific to OpenShifta such as a NetworkPolicy, SecurityContext, and Route. By default, the server.dev.enabled parameter is set to false. To enable the dev mode for the Vault server, you can experiment with Vault without needing to unseal. A Vault server normally starts in a sealed state. A sealed Vault server can access the physical storage, but cannot decrypt the data. When you unseal Vault, you allow access by giving the plaintext master key necessary to read the decryption key to decrypt the data. The authDelegator.enabled parameter binds a Cluster Role Binding to the Vault ServiceAccount. This Cluster Role Binding has the necessary privileges for Vault to use the Kubernetes Auth Method. Install the Vault Helm chart using the values.yaml file to define custom configuration values, helm install vault hashicorp/vault -n $VAULT_NAMESPACE -f my_values.yaml Verify the installation was successful, $ oc get pods NAME READY STATUS RESTARTS AGE vault-0 1 /1 Running 0 13s vault-agent-injector-588c48db4b-h9xfv 1 /1 Running 0 13s After installation of Vault, exec into the Vault pod and configure Kubernetes authentication, $ oc exec -it vault-0 -- /bin/sh vault auth enable kubernetes JWT = $( cat /var/run/secrets/kubernetes.io/serviceaccount/token ) echo $HWT HOST = \"https:// $KUBERNETES_PORT_443_TCP_ADDR :443\" echo $HOST CA_CERT = @/var/run/secrets/kubernetes.io/serviceaccount/ca.crt echo $CA_CERT vault write auth/kubernetes/config token_reviewer_jwt = $JWT kubernetes_host = $HOST kubernetes_ca_cert = $CA_CERT Vault Agent \u00b6 The Vault Agent performs three functions: It authenticates with Vault using the Kubernetes authentication method. It stores the Vault token in a sink file like /var/run/secrets/vaultproject.io/token , and keeps it valid by refreshing it at the appropriate time. The template allows Vault secrets to be rendered to files using Consul Template markup. The Vault Agent runs as an init sidecar container and shares an in-memory volume in which the token is retrieved. The shared memory volume is mounted to /vault/secrets and used by the Vault Agent containers for sharing secrets with the other containers in the pod. source: https://www.openshift.com/blog/integrating-hashicorp-vault-in-openshift-4 The Mutating Webhook Vault Agent Sidecar Injector automatically injects sidecar containers using a Kubernetes mutating admission controller. The vault-k8s binary integrates Vault and Kubernetes. The Vault Agent Injector works by intercepting pod CREATE and UPDATE events in Kubernetes. The controller parses the event and looks for the metadata annotation vault.hashicorp.com/agent-inject: true . If found, the controller will alter the pod specification based on other annotations present. Two types of Vault Agent containers can be injected: init and sidecar. The init container will prepopulate the shared memory volume with the requested secrets prior to the other containers starting. The sidecar container will continue to authenticate and render secrets to the same location as the pod runs. Using annotations, the initialization and sidecar containers may be disabled. two additional types of volumes can be optionally mounted to the Vault Agent containers. The first is secret volume containing TLS requirements such as client and CA (certificate authority) certificates and keys. The second is a configuration map containing Vault Agent configuration files. This volume is useful to customize Vault Agent beyond what the provided annotations offer. The primary method of authentication with Vault when using the Vault Agent Injector is the service account attached to the pod. For Kubernetes authentication, the service account must be bound to a Vault role and a policy granting access to the secrets desired. One of two methods of configuring the Vault Agent containers to render secrets can be used: the vault.hashicorp.com/agent-inject-secret annotation, used in this tutorial, or a configuration map containing Vault Agent configuration files. To configure secret injection, add: one or more secret annotations, and the Vault role for accessing the secrets. The annotation must have the format: vault.hashicorp.com/agent-inject-secret-<unique-name> : /path/to/secret source: https://www.openshift.com/blog/integrating-hashicorp-vault-in-openshift-4 PostStart \u00b6 You could alternatively have included the post installation configuration commands in a postStart hook of the values.yaml, postStart : - /bin/sh - -ec - > sleep 5; vault auth enable kubernetes; vault write auth/kubernetes/config token_reviewer_jwt=\"$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)\" kubernetes_host=\"https://$KUBERNETES_PORT_443_TCP_ADDR:443\" kubernetes_ca_cert=@/var/run/secrets/kubernetes.io/serviceaccount/ca.crt Container lifecycle hooks in Kubernetes run code implemented in a handler that are triggered by container lifecycle events. Two container hooks are exposed to containers: PostStart and PreStop . The PostStart parameter enables the Kubernetes postStart hook, which executes commands immediately after a container is created, but there is no guarantee that the hook will execute before the container ENTRYPOINT. A related PreStop hook is executed before a container is terminated due to an API request or management event. If either a PostStart or PreStop hook fails, it kills the Container. If a handler fails for some reason, it broadcasts an event. For PostStart, this is the FailedPostStartHook event, and for PreStop, this is the FailedPreStopHook event. For more detail, see the Termination of Pods . Congrats! You have installed Vault and the Vault agent injector. You are ready to consume encrypted secrets in your app.","title":"Lab2. Setup Internal Vault"},{"location":"vault/2_setup_internal_vault/#lab2-setup-internal-vault-with-agent-injector-on-openshift","text":"","title":"Lab2: Setup Internal Vault with Agent Injector on OpenShift"},{"location":"vault/2_setup_internal_vault/#setup-helm-v3","text":"In the IBM Cloud shell create an alias for Helm v3, alias helm = helm3","title":"Setup Helm v3"},{"location":"vault/2_setup_internal_vault/#connect-to-openshift","text":"oc login --token = <your-openshift-token> --server = <your-openshift-cluster-url> Create a new project or namespace to install Vault, VAULT_NAMESPACE = my-vault-0 oc new-project $VAULT_NAMESPACE","title":"Connect to OpenShift"},{"location":"vault/2_setup_internal_vault/#install-vault-using-helm-chart","text":"The recommended way to run Vault on OpenShift is using the Helm Chart. helm repo add hashicorp https://helm.releases.hashicorp.com Create a custom values.yaml file to set custom defaults for the Vault Helm chart. The full default values.yaml for Vault Helm chart is found at hashicorp/vault-helm . In the postStart hook, in Vault enable Kubernetes authentication and configure Vault to securely communicate with OpenShift. echo '# Custom values for the Vault chart global: # If deploying to OpenShift openshift: true server: dev: enabled: true serviceAccount: create: true name: vault-sa injector: enabled: true authDelegator: enabled: true' > my_values.yaml By default, the injector.enabled parameter is set to true . So together with Vault, the Helm chart installed a Vault Agent Injector Admission Webhook Controller in Kubernetes. By default, the global.openshift parameter is set to false . If set to true , it enables configuration specific to OpenShifta such as a NetworkPolicy, SecurityContext, and Route. By default, the server.dev.enabled parameter is set to false. To enable the dev mode for the Vault server, you can experiment with Vault without needing to unseal. A Vault server normally starts in a sealed state. A sealed Vault server can access the physical storage, but cannot decrypt the data. When you unseal Vault, you allow access by giving the plaintext master key necessary to read the decryption key to decrypt the data. The authDelegator.enabled parameter binds a Cluster Role Binding to the Vault ServiceAccount. This Cluster Role Binding has the necessary privileges for Vault to use the Kubernetes Auth Method. Install the Vault Helm chart using the values.yaml file to define custom configuration values, helm install vault hashicorp/vault -n $VAULT_NAMESPACE -f my_values.yaml Verify the installation was successful, $ oc get pods NAME READY STATUS RESTARTS AGE vault-0 1 /1 Running 0 13s vault-agent-injector-588c48db4b-h9xfv 1 /1 Running 0 13s After installation of Vault, exec into the Vault pod and configure Kubernetes authentication, $ oc exec -it vault-0 -- /bin/sh vault auth enable kubernetes JWT = $( cat /var/run/secrets/kubernetes.io/serviceaccount/token ) echo $HWT HOST = \"https:// $KUBERNETES_PORT_443_TCP_ADDR :443\" echo $HOST CA_CERT = @/var/run/secrets/kubernetes.io/serviceaccount/ca.crt echo $CA_CERT vault write auth/kubernetes/config token_reviewer_jwt = $JWT kubernetes_host = $HOST kubernetes_ca_cert = $CA_CERT","title":"Install Vault using Helm Chart"},{"location":"vault/2_setup_internal_vault/#vault-agent","text":"The Vault Agent performs three functions: It authenticates with Vault using the Kubernetes authentication method. It stores the Vault token in a sink file like /var/run/secrets/vaultproject.io/token , and keeps it valid by refreshing it at the appropriate time. The template allows Vault secrets to be rendered to files using Consul Template markup. The Vault Agent runs as an init sidecar container and shares an in-memory volume in which the token is retrieved. The shared memory volume is mounted to /vault/secrets and used by the Vault Agent containers for sharing secrets with the other containers in the pod. source: https://www.openshift.com/blog/integrating-hashicorp-vault-in-openshift-4 The Mutating Webhook Vault Agent Sidecar Injector automatically injects sidecar containers using a Kubernetes mutating admission controller. The vault-k8s binary integrates Vault and Kubernetes. The Vault Agent Injector works by intercepting pod CREATE and UPDATE events in Kubernetes. The controller parses the event and looks for the metadata annotation vault.hashicorp.com/agent-inject: true . If found, the controller will alter the pod specification based on other annotations present. Two types of Vault Agent containers can be injected: init and sidecar. The init container will prepopulate the shared memory volume with the requested secrets prior to the other containers starting. The sidecar container will continue to authenticate and render secrets to the same location as the pod runs. Using annotations, the initialization and sidecar containers may be disabled. two additional types of volumes can be optionally mounted to the Vault Agent containers. The first is secret volume containing TLS requirements such as client and CA (certificate authority) certificates and keys. The second is a configuration map containing Vault Agent configuration files. This volume is useful to customize Vault Agent beyond what the provided annotations offer. The primary method of authentication with Vault when using the Vault Agent Injector is the service account attached to the pod. For Kubernetes authentication, the service account must be bound to a Vault role and a policy granting access to the secrets desired. One of two methods of configuring the Vault Agent containers to render secrets can be used: the vault.hashicorp.com/agent-inject-secret annotation, used in this tutorial, or a configuration map containing Vault Agent configuration files. To configure secret injection, add: one or more secret annotations, and the Vault role for accessing the secrets. The annotation must have the format: vault.hashicorp.com/agent-inject-secret-<unique-name> : /path/to/secret source: https://www.openshift.com/blog/integrating-hashicorp-vault-in-openshift-4","title":"Vault Agent"},{"location":"vault/2_setup_internal_vault/#poststart","text":"You could alternatively have included the post installation configuration commands in a postStart hook of the values.yaml, postStart : - /bin/sh - -ec - > sleep 5; vault auth enable kubernetes; vault write auth/kubernetes/config token_reviewer_jwt=\"$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)\" kubernetes_host=\"https://$KUBERNETES_PORT_443_TCP_ADDR:443\" kubernetes_ca_cert=@/var/run/secrets/kubernetes.io/serviceaccount/ca.crt Container lifecycle hooks in Kubernetes run code implemented in a handler that are triggered by container lifecycle events. Two container hooks are exposed to containers: PostStart and PreStop . The PostStart parameter enables the Kubernetes postStart hook, which executes commands immediately after a container is created, but there is no guarantee that the hook will execute before the container ENTRYPOINT. A related PreStop hook is executed before a container is terminated due to an API request or management event. If either a PostStart or PreStop hook fails, it kills the Container. If a handler fails for some reason, it broadcasts an event. For PostStart, this is the FailedPostStartHook event, and for PreStop, this is the FailedPreStopHook event. For more detail, see the Termination of Pods . Congrats! You have installed Vault and the Vault agent injector. You are ready to consume encrypted secrets in your app.","title":"PostStart"},{"location":"vault/3_access_internal_vault_using_injector_annotations/","text":"Access Vault using Agent Injector Annotations \u00b6 If you have Vault and the Vault Agent Injector installed on OpenShift, you can deploy an application and configure the application to have read access to the Vault instance. Create a Namespace \u00b6 Create a new project or namespace to install the app, #APP_NAMESPACE=my-guestbook #oc new-project $APP_NAMESPACE oc label namespace $VAULT_NAMESPACE vault.hashicorp.com/agent-webhook = enabled Enable \u00b6 $ oc exec -it vault-0 -- /bin/sh vault secrets enable -path = internal kv-v2 Create a Vault Policy \u00b6 You need write access to create the policy file. To write to file, change to the current user $HOME directory. Then create the policy in Vault. cd $HOME echo 'path \"internal/data/mongodb/username-password\" { capabilities = [\"read\"] }' > my_policy.hcl vault policy write guestbook my_policy.hcl Create Authentication Role to Allow Read Access to ServiceAccount in Namespace \u00b6 In the Vault pod, create the Role to allow the ServiceAccount in the given namespace assigning read access. $ oc exec -it vault-0 -- env NS = $VAULT_NAMESPACE /bin/sh APP_SA = vault-sa APP_NAMESPACE = $NS APP_ROLE_NAME = guestbook VAULT_POLICY_NAME = guestbook vault write auth/kubernetes/role/ $APP_ROLE_NAME bound_service_account_names = $APP_SA bound_service_account_namespaces = $APP_NAMESPACE policies = $VAULT_POLICY_NAME ttl = 24h Create a Static Secret \u00b6 Create a static secret at path guestbook/mongodb/username-password with a mongo-username and mongo-password . USERNAME = admin PASSWORD = Passw0rd1 vault kv put internal/mongodb/username-password username = $USERNAME password = $PASSWORD Retrieve the secret using, / $ vault kv get internal/mongodb/username-password ====== Metadata ====== Key Value --- ----- created_time 2021 -03-06T00:32:59.879240011Z deletion_time n/a destroyed false version 1 ====== Data ====== Key Value --- ----- password Passw0rd1 username admin / $ exit Deploy Guestbook \u00b6 Create a secret to pull the image from a private repo on quay.io, REPO_USERNAME = remkohdev_us #<registry_username> REPO_PASSWORD = <registry_password> REPO_EMAIL = remkohdev@us.ibm.com #<registry_email> oc create secret docker-registry quayiocred --docker-server = quay.io --docker-username = $REPO_USERNAME --docker-password = $REPO_PASSWORD --docker-email = $REPO_EMAIL echo '--- apiVersion: apps/v1 kind: Deployment metadata: name: guestbook labels: app: guestbook spec: replicas: 1 selector: matchLabels: app: guestbook template: metadata: annotations: labels: app: guestbook spec: serviceAccountName: vault-sa containers: - name: guestbook image: quay.io/remkohdev_us/guestbook-nodejs:1.0.0 imagePullPolicy: Always ports: - name: http containerPort: 3000 imagePullSecrets: - name: quayiocred' > guestbook-deployment.yaml echo '--- apiVersion : v1 kind : Service metadata : name : guestbook labels : app : guestbook spec : type : LoadBalancer ports : - port : 80 targetPort : 3000 name : http selector : app : guestbook' > guestbook-service.yaml oc create -f guestbook-deployment.yaml oc create -f guestbook-service.yaml oc expose service guestbook ROUTE = $( oc get route guestbook -o json | jq -r '.spec.host' ) echo $ROUTE Test the deployment, curl -X POST http:// $ROUTE /api/entries -H 'Content-Type: application/json' -H 'Accept: application/json' -d '{ \"message\": \"hello1\" }' Deploy the App using Vault Annotation \u00b6 Create a patch definition for the Guestbook deployment using Vault annotations, echo '--- spec: template: metadata: annotations: vault.hashicorp.com/agent-inject: \"true\" vault.hashicorp.com/role: \"guestbook\" vault.hashicorp.com/agent-inject-secret-guestbook-db-conf.txt: \"internal/data/mongodb/username-password\"' > guestbook-deployment-patch.yaml Apply the patch, oc patch deployment guestbook --patch \" $( cat guestbook-deployment-patch.yaml ) \" oc get pods $ oc exec $( oc get pod --selector = 'app=guestbook' --output = 'jsonpath={.items[0].metadata.name}' ) --container guestbook -- cat /vault/secrets/guestbook-db-conf.txt data: map [ password:Passw0rd1 username:admin ] metadata: map [ created_time:2021-04-04T13:52:59.891946224Z deletion_time: destroyed:false version:1 ]","title":"Lab3. Vault Agent Injector"},{"location":"vault/3_access_internal_vault_using_injector_annotations/#access-vault-using-agent-injector-annotations","text":"If you have Vault and the Vault Agent Injector installed on OpenShift, you can deploy an application and configure the application to have read access to the Vault instance.","title":"Access Vault using Agent Injector Annotations"},{"location":"vault/3_access_internal_vault_using_injector_annotations/#create-a-namespace","text":"Create a new project or namespace to install the app, #APP_NAMESPACE=my-guestbook #oc new-project $APP_NAMESPACE oc label namespace $VAULT_NAMESPACE vault.hashicorp.com/agent-webhook = enabled","title":"Create a Namespace"},{"location":"vault/3_access_internal_vault_using_injector_annotations/#enable","text":"$ oc exec -it vault-0 -- /bin/sh vault secrets enable -path = internal kv-v2","title":"Enable"},{"location":"vault/3_access_internal_vault_using_injector_annotations/#create-a-vault-policy","text":"You need write access to create the policy file. To write to file, change to the current user $HOME directory. Then create the policy in Vault. cd $HOME echo 'path \"internal/data/mongodb/username-password\" { capabilities = [\"read\"] }' > my_policy.hcl vault policy write guestbook my_policy.hcl","title":"Create a Vault Policy"},{"location":"vault/3_access_internal_vault_using_injector_annotations/#create-authentication-role-to-allow-read-access-to-serviceaccount-in-namespace","text":"In the Vault pod, create the Role to allow the ServiceAccount in the given namespace assigning read access. $ oc exec -it vault-0 -- env NS = $VAULT_NAMESPACE /bin/sh APP_SA = vault-sa APP_NAMESPACE = $NS APP_ROLE_NAME = guestbook VAULT_POLICY_NAME = guestbook vault write auth/kubernetes/role/ $APP_ROLE_NAME bound_service_account_names = $APP_SA bound_service_account_namespaces = $APP_NAMESPACE policies = $VAULT_POLICY_NAME ttl = 24h","title":"Create Authentication Role to Allow Read Access to ServiceAccount in Namespace"},{"location":"vault/3_access_internal_vault_using_injector_annotations/#create-a-static-secret","text":"Create a static secret at path guestbook/mongodb/username-password with a mongo-username and mongo-password . USERNAME = admin PASSWORD = Passw0rd1 vault kv put internal/mongodb/username-password username = $USERNAME password = $PASSWORD Retrieve the secret using, / $ vault kv get internal/mongodb/username-password ====== Metadata ====== Key Value --- ----- created_time 2021 -03-06T00:32:59.879240011Z deletion_time n/a destroyed false version 1 ====== Data ====== Key Value --- ----- password Passw0rd1 username admin / $ exit","title":"Create a Static Secret"},{"location":"vault/3_access_internal_vault_using_injector_annotations/#deploy-guestbook","text":"Create a secret to pull the image from a private repo on quay.io, REPO_USERNAME = remkohdev_us #<registry_username> REPO_PASSWORD = <registry_password> REPO_EMAIL = remkohdev@us.ibm.com #<registry_email> oc create secret docker-registry quayiocred --docker-server = quay.io --docker-username = $REPO_USERNAME --docker-password = $REPO_PASSWORD --docker-email = $REPO_EMAIL echo '--- apiVersion: apps/v1 kind: Deployment metadata: name: guestbook labels: app: guestbook spec: replicas: 1 selector: matchLabels: app: guestbook template: metadata: annotations: labels: app: guestbook spec: serviceAccountName: vault-sa containers: - name: guestbook image: quay.io/remkohdev_us/guestbook-nodejs:1.0.0 imagePullPolicy: Always ports: - name: http containerPort: 3000 imagePullSecrets: - name: quayiocred' > guestbook-deployment.yaml echo '--- apiVersion : v1 kind : Service metadata : name : guestbook labels : app : guestbook spec : type : LoadBalancer ports : - port : 80 targetPort : 3000 name : http selector : app : guestbook' > guestbook-service.yaml oc create -f guestbook-deployment.yaml oc create -f guestbook-service.yaml oc expose service guestbook ROUTE = $( oc get route guestbook -o json | jq -r '.spec.host' ) echo $ROUTE Test the deployment, curl -X POST http:// $ROUTE /api/entries -H 'Content-Type: application/json' -H 'Accept: application/json' -d '{ \"message\": \"hello1\" }'","title":"Deploy Guestbook"},{"location":"vault/3_access_internal_vault_using_injector_annotations/#deploy-the-app-using-vault-annotation","text":"Create a patch definition for the Guestbook deployment using Vault annotations, echo '--- spec: template: metadata: annotations: vault.hashicorp.com/agent-inject: \"true\" vault.hashicorp.com/role: \"guestbook\" vault.hashicorp.com/agent-inject-secret-guestbook-db-conf.txt: \"internal/data/mongodb/username-password\"' > guestbook-deployment-patch.yaml Apply the patch, oc patch deployment guestbook --patch \" $( cat guestbook-deployment-patch.yaml ) \" oc get pods $ oc exec $( oc get pod --selector = 'app=guestbook' --output = 'jsonpath={.items[0].metadata.name}' ) --container guestbook -- cat /vault/secrets/guestbook-db-conf.txt data: map [ password:Passw0rd1 username:admin ] metadata: map [ created_time:2021-04-04T13:52:59.891946224Z deletion_time: destroyed:false version:1 ]","title":"Deploy the App using Vault Annotation"}]}